{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {},
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Analysis \u2014 Amazon Fine Food Reviews\n",
        "This notebook performs preprocessing, TF-IDF, Word2Vec, GloVe embeddings, classical ML models, and BiLSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data\n",
        "df = pd.read_csv('Reviews.csv')\n",
        "df = df[['Text','Score']].dropna()\n",
        "\n",
        "def label(x):\n",
        "    if x <= 2: return 'negative'\n",
        "    if x == 3: return 'neutral'\n",
        "    return 'positive'\n",
        "\n",
        "df['sentiment'] = df['Score'].apply(label)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemm = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemm.lemmatize(w) for w in tokens if w not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['clean'] = df['Text'].astype(str).apply(preprocess)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train-test split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "y_train = train['sentiment']\n",
        "y_test = test['sentiment']\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=20000)\n",
        "X_train = vectorizer.fit_transform(train['clean'])\n",
        "X_test = vectorizer.transform(test['clean'])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Logistic Regression baseline\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    }
  ]
}